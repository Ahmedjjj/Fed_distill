# dataset-distillation


This repository contains the code for a project done at the MLO Lab, EPFL, Switzerland.
The title of the project is: *Dataset Distillation for One-Shot Federated Learning*

Please read the report (`report.pdf`) for more details.

# Requirements
The file `requirements.txt` contains the necessary libraries to run the code, with the versions with which we ran the code. We use python `3.9.7`. 
You may install the necessay libraries using:\\
```bash
pip install -e requirements.txt
```
However, this may not be a good idea, since pytorch may need different versions depending on different cuda versions. A good way to simplify this process is to use [light-the-torch](https://github.com/pmeier/light-the-torch).

# Repository structure
The code is under `fed_distill`, and is fairly well documented.
# Reproducibility of experiments

# Acknowledgments